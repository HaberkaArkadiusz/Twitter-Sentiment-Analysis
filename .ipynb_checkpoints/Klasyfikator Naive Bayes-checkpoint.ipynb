{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9223372036854775807"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, platform\n",
    "platform.architecture()\n",
    "sys.maxsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import\n",
    "test_tweets = pd.read_csv('test_tweets.csv')\n",
    "train_tweets = pd.read_csv('train_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Czyszczenie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Usuwa wszystkie znaki oprócz a-z A-Z i spacji\n",
    "def leave_alphas(x):\n",
    "    filtered = ''\n",
    "    for char in x:\n",
    "        if((ord(char) >= 97 and ord(char) <= 122) or (ord(char) >= 65 and ord(char) <= 90 or ord(char) == 32)):\n",
    "            filtered += char\n",
    "    return filtered\n",
    "\n",
    "train_tweets['tweet'] = train_tweets['tweet'].apply(leave_alphas)\n",
    "test_tweets['tweet'] = test_tweets['tweet'].apply(leave_alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zamiana 0 - 1 na pos - neg\n",
    "def format_pos_neg(x):\n",
    "    if x == 1:\n",
    "        return 'neg'\n",
    "    if x == 0:\n",
    "        return 'pos'\n",
    "\n",
    "train_tweets['label'] = train_tweets['label'].apply(format_pos_neg)\n",
    "\n",
    "# drop id\n",
    "train_tweets.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# zamiana kolejnosci kolumn\n",
    "cols = train_tweets.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "train_tweets = train_tweets[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Partycjonowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10654"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part = int(train_tweets['tweet'].count() / 3)\n",
    "part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set1: 10654 Set2: 10654 Set3: 10654\n"
     ]
    }
   ],
   "source": [
    "set1 = train_tweets.iloc[0:part, :]\n",
    "set2 = train_tweets.iloc[part:part*2, :]\n",
    "set3 = train_tweets.iloc[part*2:, :]\n",
    "print(\"Set1: \" + str(set1['tweet'].count()) + \" Set2: \" + str(set2['tweet'].count()) + \" Set3: \" + str(set3['tweet'].count())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train obs: 7457 Test obs: 3197\n"
     ]
    }
   ],
   "source": [
    "train_numb = int(part * 0.7)\n",
    "test_numb = part - train_numb\n",
    "print(\" Train obs: \" + str(train_numb) + \" Test obs: \" + str(test_numb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set1train: 7457 Set1test: 3197\n"
     ]
    }
   ],
   "source": [
    "set1train = set1.iloc[0:train_numb,:]\n",
    "set1test = set1.iloc[train_numb:,:]\n",
    "\n",
    "set2train = set2.iloc[0:train_numb,:]\n",
    "set2test = set2.iloc[train_numb:,:]\n",
    "\n",
    "set3train = set3.iloc[0:train_numb,:]\n",
    "set3test = set3.iloc[train_numb:,:]\n",
    "\n",
    "print(\"Set1train: \" + str(set1train['tweet'].count()) + \" Set1test: \" + str(set1test['tweet'].count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Przeformatowanie na tablice tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset = set1train[['tweet','label']]\n",
    "set1train = [tuple(x) for x in set1train.values]\n",
    "\n",
    "subset = set1test[['tweet','label']]\n",
    "set1test = [tuple(x) for x in set1test.values]\n",
    "#\n",
    "subset = set2train[['tweet','label']]\n",
    "set2train = [tuple(x) for x in set2train.values]\n",
    "\n",
    "subset = set2test[['tweet','label']]\n",
    "set2test = [tuple(x) for x in set2test.values]\n",
    "#\n",
    "subset = set3train[['tweet','label']]\n",
    "set3train = [tuple(x) for x in set3train.values]\n",
    "\n",
    "subset = set3test[['tweet','label']]\n",
    "set3test = [tuple(x) for x in set3test.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Trening danych na klasyfikatorze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob.classifiers import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-bab073a8bd88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Utworzenie obiektu klasyfikatora na podstawie set1train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNaiveBayesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset1train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\textblob\\classifiers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, train_set, feature_extractor, format, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m                  feature_extractor=basic_extractor, format=None, **kwargs):\n\u001b[0;32m    205\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNLTKClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\textblob\\classifiers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    204\u001b[0m                  feature_extractor=basic_extractor, format=None, **kwargs):\n\u001b[0;32m    205\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNLTKClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\textblob\\classifiers.py\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Feature extractor may take one or two arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\textblob\\classifiers.py\u001b[0m in \u001b[0;36mbasic_extractor\u001b[1;34m(document, train_set)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_document_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     features = dict(((u'contains({0})'.format(word), (word in tokens))\n\u001b[1;32m---> 97\u001b[1;33m                                             for word in word_features))\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Utworzenie obiektu klasyfikatora na podstawie set1train\n",
    "cl = NaiveBayesClassifier(set1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9433844228964654"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ocena trafności na set1test\n",
    "cl.accuracy(set1test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "       contains(libtard) = True              neg : pos    =    155.0 : 1.0\n",
      "        contains(racism) = True              neg : pos    =    128.5 : 1.0\n",
      "      contains(equality) = True              neg : pos    =     84.2 : 1.0\n",
      "         contains(bigot) = True              neg : pos    =     84.2 : 1.0\n",
      "        contains(racist) = True              neg : pos    =     69.4 : 1.0\n",
      "           contains(blm) = True              neg : pos    =     66.4 : 1.0\n",
      "       contains(liberal) = True              neg : pos    =     51.9 : 1.0\n",
      "      contains(hispanic) = True              neg : pos    =     48.7 : 1.0\n",
      "          contains(tcot) = True              neg : pos    =     39.9 : 1.0\n",
      "       contains(african) = True              neg : pos    =     39.9 : 1.0\n",
      "         contains(tampa) = True              neg : pos    =     39.9 : 1.0\n",
      "      contains(feminism) = True              neg : pos    =     39.9 : 1.0\n",
      "       contains(boricua) = True              neg : pos    =     39.9 : 1.0\n",
      "            contains(mc) = True              neg : pos    =     39.9 : 1.0\n",
      "         contains(white) = True              neg : pos    =     35.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "cl.show_informative_features(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-3887e7312dcc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset1test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\textblob\\classifiers.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, new_data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_words_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         self.train_features = [(self.extract_features(d), c)\n\u001b[1;32m--> 279\u001b[1;33m                                 for d, c in self.train_set]\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             self.classifier = self.nltk_class.train(self.train_features,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\textblob\\classifiers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_words_from_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m         self.train_features = [(self.extract_features(d), c)\n\u001b[1;32m--> 279\u001b[1;33m                                 for d, c in self.train_set]\n\u001b[0m\u001b[0;32m    280\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             self.classifier = self.nltk_class.train(self.train_features,\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\textblob\\classifiers.py\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;31m# Feature extractor may take one or two arguments\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_word_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\textblob\\classifiers.py\u001b[0m in \u001b[0;36mbasic_extractor\u001b[1;34m(document, train_set)\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_document_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     features = dict(((u'contains({0})'.format(word), (word in tokens))\n\u001b[1;32m---> 97\u001b[1;33m                                             for word in word_features))\n\u001b[0m\u001b[0;32m     98\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cl.update(set1test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "# NOT IMPLEMENTED:\n",
    "def cleandata(x):\n",
    "    obj = TextBlob(x)\n",
    "    new_blob = []\n",
    "    for word in obj.words:\n",
    "        if word != TextBlob.Word('user'):\n",
    "            new_blob.append(word)\n",
    "    result = \" \".join(str(x) for x in new_blob)\n",
    "    return result\n",
    "\n",
    "train_tweets['tweet'] = train_tweets['tweet'].apply(cleandata)\n",
    "test_tweets['tweet'] = test_tweets['tweet'].apply(cleandata)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
